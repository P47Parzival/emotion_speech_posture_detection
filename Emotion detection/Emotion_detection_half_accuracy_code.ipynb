{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8bc8273e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage import feature\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "db67c623"
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "def get_lbp_features(image):\n",
    "    lbp = feature.local_binary_pattern(image, P=24, R=8, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 27), range=(0, 26))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ebc46389"
   },
   "outputs": [],
   "source": [
    "def get_hog_features(image):\n",
    "    features = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                   cells_per_block=(2, 2), block_norm=\"L2-Hys\", visualize=False)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2a44ee02"
   },
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for img in tqdm(images, desc=\"Extracting Features\"):\n",
    "        lbp_feat = get_lbp_features(img)\n",
    "        hog_feat = get_hog_features(img)\n",
    "        combined = np.hstack([lbp_feat, hog_feat])\n",
    "        features.append(combined)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e1192ae6"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(\"fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2fbc5621"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 35887/35887 [00:22<00:00, 1588.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "emotion_labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy',\n",
    "                  4: 'sad', 5: 'surprise', 6: 'neutral'}\n",
    "\n",
    "tqdm.pandas()\n",
    "df['image'] = df['pixels'].progress_apply(lambda x: np.array(x.split(' '), 'float32').reshape(48, 48))\n",
    "X = np.array(df['image'].tolist())\n",
    "y = np.array(df['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "60900fcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   0%|                                                                   | 0/35887 [00:00<?, ?it/s]C:\\Users\\dhruv\\anaconda3\\Lib\\site-packages\\skimage\\feature\\texture.py:360: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n",
      "Extracting Features: 100%|██████████████████████████████████████████████████████| 35887/35887 [02:19<00:00, 256.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# extracting features\n",
    "X_features = extract_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e9b05671"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "68f19e1b"
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8eb9d5ae"
   },
   "outputs": [],
   "source": [
    "# balancing the data with SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c58ee55f"
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=150, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_bal)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7be3c661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data loaded from 'preprocessed_data.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data to avoid re-running earlier steps\n",
    "# joblib.dump((X_train_pca, y_train_bal, X_test_pca, y_test), 'preprocessed_data.pkl')\n",
    "# print(\"Preprocessed data saved as 'preprocessed_data.pkl'\")\n",
    "\n",
    "import joblib\n",
    "X_train_pca, y_train_bal, X_test_pca, y_test = joblib.load('preprocessed_data.pkl')\n",
    "print(\"Preprocessed data loaded from 'preprocessed_data.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Estimators:   0%|                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Estimators:  50%|███████████████████████████████▌                               | 1/2 [00:16<00:16, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Estimators: 100%|███████████████████████████████████████████████████████████████| 2/2 [00:57<00:00, 28.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model and scaler saved as 'ensemble_model_new.pkl' and 'scaler_new.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Ensemble model (without SVM)\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.2, random_state=42, n_jobs=-1, verbosity=1)\n",
    "rf = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train each model with tqdm\n",
    "estimators = [('XGradient Boosting', xgb), ('Random Forest', rf)]\n",
    "print(\"Training ensemble model...\")\n",
    "for name, estimator in tqdm(estimators, desc=\"Training Estimators\"):\n",
    "    print(f\"Training {name}...\")\n",
    "    estimator.fit(X_train_pca, y_train_bal)\n",
    "\n",
    "# Create VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[('xgb', xgb), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Save ensemble and scaler and pca\n",
    "joblib.dump(ensemble, 'ensemble_model_new.pkl')\n",
    "joblib.dump(scaler, 'scaler_new.pkl')\n",
    "joblib.dump(pca, 'pca_new.pkl')\n",
    "print(\"Ensemble model and scaler saved as 'ensemble_model_new.pkl' and 'scaler_new.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data\n",
      "X_test_pca shape: (7178, 150), y_test shape: (7178,)\n",
      "Loaded ensemble model\n",
      "Ensemble estimators: [('xgb', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              feature_weights=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
      "              n_jobs=-1, num_parallel_tree=None, ...)), ('rf', RandomForestClassifier(class_weight='balanced', n_estimators=300, n_jobs=-1,\n",
      "                       random_state=42))]\n",
      "Has fitted estimators_: False\n",
      "XGBoost fitted: True\n",
      "Random Forest fitted: True\n",
      "VotingClassifier initialized for soft voting\n",
      "\n",
      "Ensemble Model Accuracy: 50.67%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.41      0.39      0.40       991\n",
      "     disgust       0.59      0.38      0.46       109\n",
      "        fear       0.42      0.34      0.37      1024\n",
      "       happy       0.63      0.72      0.67      1798\n",
      "         sad       0.38      0.40      0.39      1216\n",
      "    surprise       0.65      0.66      0.65       800\n",
      "     neutral       0.45      0.45      0.45      1240\n",
      "\n",
      "    accuracy                           0.51      7178\n",
      "   macro avg       0.51      0.48      0.49      7178\n",
      "weighted avg       0.50      0.51      0.50      7178\n",
      "\n",
      "Fitted ensemble model saved as 'ensemble_model_fitted.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy',\n",
    "                  4: 'sad', 5: 'surprise', 6: 'neutral'}\n",
    "\n",
    "# Load preprocessed data\n",
    "try:\n",
    "    X_train_pca, y_train_bal, X_test_pca, y_test = joblib.load('preprocessed_data.pkl')\n",
    "    print(\"Loaded preprocessed data\")\n",
    "    print(f\"X_test_pca shape: {X_test_pca.shape}, y_test shape: {y_test.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'preprocessed_data.pkl' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Load the ensemble model\n",
    "try:\n",
    "    ensemble = joblib.load('ensemble_model_new.pkl')\n",
    "    print(\"Loaded ensemble model\")\n",
    "    print(\"Ensemble estimators:\", ensemble.estimators)\n",
    "    print(\"Has fitted estimators_:\", hasattr(ensemble, 'estimators_'))\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ensemble_model_new.pkl' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Check if estimators are fitted\n",
    "xgb, rf = ensemble.estimators[0][1], ensemble.estimators[1][1]\n",
    "xgb_fitted = hasattr(xgb, 'get_booster') and xgb.get_booster().num_boosted_rounds() > 0\n",
    "rf_fitted = hasattr(rf, 'estimators_') and len(rf.estimators_) > 0\n",
    "print(\"XGBoost fitted:\", xgb_fitted)\n",
    "print(\"Random Forest fitted:\", rf_fitted)\n",
    "\n",
    "# If not fitted, raise an error (retraining should be done separately)\n",
    "if not (xgb_fitted and rf_fitted):\n",
    "    print(\"Error: One or both estimators are not fitted. Please retrain using the training code.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize VotingClassifier for soft voting (quick, ~1-5 seconds)\n",
    "ensemble.fit(X_train_pca, y_train_bal)\n",
    "print(\"VotingClassifier initialized for soft voting\")\n",
    "\n",
    "# Evaluation\n",
    "y_pred = ensemble.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nEnsemble Model Accuracy: {accuracy*100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=emotion_labels.values()))\n",
    "\n",
    "# Save the properly fitted ensemble\n",
    "joblib.dump(ensemble, 'ensemble_model_fitted.pkl')\n",
    "print(\"Fitted ensemble model saved as 'ensemble_model_fitted.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
